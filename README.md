## TODO
### model
- [ ] PyTorch实现Transformer
- [ ] PyTorch实现Unet 
- [x] PyTorch实现Bert 详见`/BERT`

### funtion/class
- [x] Datasets.map 详见`/map/test.ipynb`
- [ ] 多卡并行训练DP、DDP、Trainer  详见`/trainer`
- [x] nn.Module 详见`/module`
- [x] gradio 详见 `/gradio_base`
- [x] dataloader与dataset 详见`data_loader.py`
- [x] yeild 详见`yeild.py`

### others
- [x] web crawler 详见`/crawler`



### LLM Tuning
| Title of the Paper | Authors | Institution | Link | 	Problem | 	Advantage | Key  | Skimming/In-depth Reading |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models](https://arxiv.org/abs/2307.07705) | Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, **Maosong Sun** | 清华大学 NLP Group, DCST, IAI, BNRIST | [[Arxiv]](https://arxiv.org/abs/2307.07705) | 以前的模型在进行模型压缩时可能会导致知识丢失和性能下降，尤其是在大型语言模型（LLMs）上，这种压缩可能会影响模型在下游任务上的表现。 | CPET模型的优势在于它结合了参数高效调整（PET）和模型压缩技术，通过知识继承和恢复策略来弥补压缩过程中可能导致的性能下降。 | 引入了两种机制：(1) PET知识继承，使用在非压缩LLM上训练的PET模块作为初始化来学习压缩LLM上的PET模块；(2) 模型知识恢复，通过在压缩LLM中添加额外的知识恢复模块来弥补压缩过程中丢失的知识。 | S |
| [![Star](https://img.shields.io/github/stars/liziniu/ReMax.svg?style=social&label=Star)](https://github.com/liziniu/ReMax) [ReMax: A Simple, Effective, and Efficient Method for Aligning Large Language Models](https://arxiv.org/abs/2310.10505) | Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo | 香港中文大学（深圳）数据科学学院，深圳大数据研究所，南京大学国家新型软件技术重点实验室，Polixir.ai，黄埔帕洲实验室（中国深圳国际工业与应用数学中心） | [[ICML 2024]](https://arxiv.org/abs/2310.10505) [[Github]](https://github.com/liziniu/ReMax) | 以前的模型，特别是PPO，对于LLMs来说过于复杂，需要大量的计算资源和内存，且超参数调整繁琐，导致在有限的计算资源下难以应用。 | ReMax模型简化了实现，减少了超参数，降低了GPU内存使用，并缩短了训练时间。它利用了RLHF的三个特性：快速模拟、确定性转换和轨迹级奖励，这些在PPO中未被充分利用。 | ReMax算法在REINFORCE算法的基础上引入了一种新的方差减少技术，并且去除了PPO算法中的价值模型部分，从而简化了算法，减少了内存消耗和训练时间。 | S |
| [TRANSOM: An Efficient Fault-Tolerant System for Training LLMs](https://arxiv.org/abs/2310.10046) | Baodong Wu, Lei Xia, Qingping Li, Kangyu Li, Xu Chen, Yongqiang Guo, Tieyao Xiang, Yuheng Chen, Shigang Li | SenseTime，Huazhong University of Science and Technology，Beijing University of Posts and Telecommunications | [[Paper]](https://arxiv.org/abs/2310.10046) | 以前的模型在训练过程中容易受到硬件和软件故障的影响，导致训练任务中断，需要耗费大量时间和资源进行任务重启和恢复。此外，传统的方法在检查点保存和加载时效率较低，增加了整体训练时间。 | TRANSOM通过设计三个关键子系统（TOL、TEE、TCE），实现了高效的容错和恢复机制。这些机制显著提升了大规模LLMs训练的效率，减少了训练中断的时间和资源消耗。 | 设计了三个关键子系统：(1) 自动容错和恢复机制的训练管道（TOL），(2) 多维度度量自动异常检测系统（TEE），(3) 训练检查点异步访问自动容错和恢复技术（TCE）。 | S |
| [DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection](https://arxiv.org/abs/2310.16776) | Devleena Das, Vivek Khetan | Georgia Institute of Technology, Accenture Labs | [[Paper]](https://arxiv.org/abs/2310.16776) | 以前的模型在微调过程中需要大量高质量数据，增加了数据获取和处理的成本。传统微调方法虽然能达到较好的性能，但在实际应用中往往面临数据不足的问题。 | DEFT-UCS通过无监督核心集选择方法，实现了在减少数据量的情况下高效微调预训练语言模型。实验结果表明，DEFT-UCS在多个文本编辑任务中，使用70%的训练数据可以达到与使用全部数据的传统方法相当的性能。 | 无监督核心集选择（UCS）方法，通过K-Means聚类选择代表性数据子集，从而减少微调所需的数据量。 | S |
| [![Star](https://img.shields.io/github/stars/yangjianxin1/LongQLoRA.svg?style=social&label=Star)](https://github.com/yangjianxin1/LongQLoRA) [LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models](https://arxiv.org/abs/2311.04879) | Jianxin Yang | Sun Yat-sen University | [[Paper]](https://arxiv.org/abs/2311.04879) [[Github]](https://github.com/yangjianxin1/LongQLoRA) | 论文试图解决大型语言模型（如LLaMA2）在有限的训练资源下，如何有效地扩展其上下文长度的问题。当前许多模型在处理超出预定义上下文长度的输入时，性能显著下降，导致在处理长文本任务（如多文档问答、书籍总结等）时表现不佳。 | 它在单个32GB V100 GPU上高效地扩展了大型语言模型（LLaMA2）的上下文长度，同时保持了良好的性能。结合了位置插值（Position Interpolation）、QLoRA和Shift Short Attention的优点，实现了计算资源的节约和性能的提升。 | 结合了位置插值（Position Interpolation）、QLoRA和Shift Short Attention。位置插值用于扩展上下文长度，QLoRA通过量化预训练模型权重和添加可学习的低秩适配器来节省GPU内存，而Shift Short Attention通过分组计算注意力来进一步节省计算资源。 | S |
| [![Star](https://img.shields.io/github/stars/ist-daslab/sparsefinetuning.svg?style=social&label=Star)](https://github.com/ist-daslab/sparsefinetuning) [Sparse Fine-tuning for Inference Acceleration of Large Language Models](https://arxiv.org/abs/2310.06927v2) | Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan Alistarh | IST Austria | [[Paper]](https://arxiv.org/abs/2310.06927v2) [[Github]](https://github.com/ist-daslab/sparsefinetuning) [[Github]](https://github.com/neuralmagic/deepsparse/tree/main/research/mpt) | 以前的模型在高稀疏度下微调时，容易出现训练不稳定的问题，导致准确性下降。此外，现有的量化方法在进一步压缩时（如到3比特）会导致准确性难以恢复，限制了推理速度的提升。 | 它能够在高效运行的同时保持高准确性。本文提出的SquareHead蒸馏方法能够在高稀疏度下进行微调，既能加速推理速度，又能保持模型的准确性，这使得模型在实践中更有用。 | 论文中解决方案的关键在于提出了一种基于L2的蒸馏方法SquareHead，通过这种方法在高稀疏度下进行微调时，能够有效地恢复模型的准确性。同时，展示了利用稀疏性实现推理加速的方法。 | S |
| [![Star](https://img.shields.io/github/stars/prateeky2806/compeft.svg?style=social&label=Star)](https://github.com/prateeky2806/compeft) [ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization](https://arxiv.org/abs/2311.13171) | Prateek Yadav, Leshem Choshen, Colin Raffel, Mohit Bansal | University of North Carolina at Chapel Hill, IBM Research, Massachusetts Institute of Technology, University of Toronto, Vector Institute | [[Paper]](https://arxiv.org/abs/2311.13171) [[Github]](https://github.com/prateeky2806/compeft) | 以前的PEFT模型虽然在参数高效微调上表现良好，但在高延迟网络环境中传输较大模型时存在明显的通信成本问题。此外，传统压缩方法在不进行额外训练的情况下，通常会导致模型性能的显著下降。 | ComPEFT模型通过稀疏化和三值量化显著减少了PEFT模型的大小，同时在多个任务和模型上的性能都保持或有所提升。压缩后的模型在高延迟网络中的传输更为高效，且在少样本组合泛化能力上表现出色。 | 关键点在于稀疏化和三值量化相结合，使得任务向量在大幅度压缩的同时仍能保持高性能。特别是选择合适的标量常数（α）用于量化，有助于恢复或提高模型性能。 | S |
| [SPT: Fine-Tuning Transformer-based Language Models Efficiently with Sparsification](https://arxiv.org/abs/2312.10365) | Yuntao Gui, Xiao Yan, Peiqi Yin, Han Yang, James Cheng | The Chinese University of Hong Kong, Southern University of Science and Technology | [[Paper]](https://arxiv.org/abs/2312.10365) [[Github]](https://github.com/ytgui/SPT-proto) | 高内存消耗：存储多头注意力机制的权重需要大量内存。计算成本高：前馈网络的计算成本高，导致运行时间长。微调效率低：直接微调大型预训练模型需要更新大量参数，效率较低。 | SPT通过引入稀疏化技术，在不显著降低模型质量的情况下，减少了内存消耗和计算成本，显著提升了微调Transformer模型的效率。 | 稀疏化技术：稀疏MHA通过只计算和存储前L个注意力权重，减少了内存消耗。路由FFN通过动态激活每个token的一部分模型参数，减少了计算成本。 | S |
| [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) | Soufiane Hayou, Nikhil Ghosh, Bin Yu | Simons Institute, UC Berkeley, Department of Statistics, UC Berkeley | [[Paper]](https://arxiv.org/abs/2402.12354) [[Github]](https://github.com/nikhil-ghosh-berkeley/loraplus) | 以前的 LoRA 模型在处理大宽度模型时效果不佳，因为它为适配器矩阵 A 和 B 设置相同的学习率，导致特征学习效率低下，微调效果不理想。 | LoRA+ 通过为 LoRA 适配器矩阵 A 和 B 设置不同的学习率，提高了特征学习效率，使得大宽度模型在微调时能够更有效地学习和适应新任务，从而在性能和速度上都有显著提升。 | 为 LoRA 适配器矩阵 A 和 B 设置不同的学习率，并通过缩放理论确定一个合适的固定比例。这种方法被称为 LoRA+，它在相同计算成本下提高了特征学习效率和模型性能。 | S |
